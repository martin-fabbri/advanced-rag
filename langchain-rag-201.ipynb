{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG 201"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentative techniques to explore:\n",
    "\n",
    "1. MultiQueryRetriever\n",
    "2. Contextual Compression\n",
    "3. Ensemble Retrievers\n",
    "4. Self-quering Retrievers\n",
    "5. Time weighted vector store retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest 7.2.1 requires attrs>=19.2.0, but you have attrs 18.2.0 which is incompatible.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tMODIFIED       \n",
      "llama2:latest\t78e26419b446\t3.8 GB\t17 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "# %> curl -fsSL https://ollama.com/install.sh | sh\n",
    "# %> ollama serve\n",
    "# %> ollama pull llama2\n",
    "# %> ollama pull nomic-embed-text\n",
    "\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Of course! Here are five interesting facts about Roman history:\n",
      "\n",
      "1. The Roman Empire was the largest empire in the world at its peak, spanning across three continents and encompassing over 20% of the world's land area.\n",
      "2. The Roman language, Latin, is still studied today and has had a profound influence on many modern languages, including English. In fact, over 75% of English words have roots in Latin.\n",
      "3. The Roman calendar, which was based on a solar year, was the precursor to the modern Gregorian calendar used today. The Romans also invented the concept of days of the week, with names like Sunday (Latin: Dies Solis) and Monday (Latin: Dies Lunae) still in use today.\n",
      "4. The Roman Colosseum, completed in 80 AD, was the largest amphitheater in the ancient world and could hold up to 50,000 spectators. It was used for gladiatorial contests, animal hunts, and other public events.\n",
      "5. The Roman Empire had a complex system of governance, with power divided between the emperor, the Senate, and the assemblies of citizens. The concept of checks and balances, which is central to modern democracy, was first developed in ancient Rome."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n Of course! Here are five interesting facts about Roman history:\\n\\n1. The Roman Empire was the largest empire in the world at its peak, spanning across three continents and encompassing over 20% of the world's land area.\\n2. The Roman language, Latin, is still studied today and has had a profound influence on many modern languages, including English. In fact, over 75% of English words have roots in Latin.\\n3. The Roman calendar, which was based on a solar year, was the precursor to the modern Gregorian calendar used today. The Romans also invented the concept of days of the week, with names like Sunday (Latin: Dies Solis) and Monday (Latin: Dies Lunae) still in use today.\\n4. The Roman Colosseum, completed in 80 AD, was the largest amphitheater in the ancient world and could hold up to 50,000 spectators. It was used for gladiatorial contests, animal hunts, and other public events.\\n5. The Roman Empire had a complex system of governance, with power divided between the emperor, the Senate, and the assemblies of citizens. The concept of checks and balances, which is central to modern democracy, was first developed in ancient Rome.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "llm(\"Tell me 5 facts about Roman history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 404, {\"error\":\"model 'nomic-embed-text' not found, try pulling it first\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-embed-text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbed this text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m embed\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain_community/embeddings/ollama.py:217\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a query using a Ollama deployed embedding model.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m instruction_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 217\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction_pair\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain_community/embeddings/ollama.py:192\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain_community/embeddings/ollama.py:192\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/langchain_community/embeddings/ollama.py:166\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API HTTP code: 404, {\"error\":\"model 'nomic-embed-text' not found, try pulling it first\"}"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "text = \"Embed this text\"\n",
    "embed = embeddings.embed_query(text)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "To generate embeddings, you can either query an invidivual text, or you can query a list of texts.\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
